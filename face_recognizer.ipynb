{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sourabh/Documents/FR/faces\n"
     ]
    }
   ],
   "source": [
    "cd /home/sourabh/Documents/FR/faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is trained now!\n"
     ]
    }
   ],
   "source": [
    "# all face images in .jpg format\n",
    "items = os.listdir()\n",
    "\n",
    "id_={\n",
    "    10:'person_1',\n",
    "    1:'Person_2'\n",
    "}\n",
    "\n",
    "# training_data and labels\n",
    "train_data,labels = [],[]\n",
    "for i, img in enumerate(items):\n",
    "    if img.endswith('.jpg') and not img.startswith('r'):\n",
    "        image = cv2.imread(img,0)\n",
    "        train_data.append(np.array(image,'uint8'))\n",
    "        labels.append(1)\n",
    "    if img.endswith('.jpg') and img.startswith('r'):\n",
    "        image = cv2.imread(img,0)\n",
    "        train_data.append(np.array(image,'uint8'))\n",
    "        labels.append(10)\n",
    "# print(labels)\n",
    "\n",
    "labels = np.asarray(labels,dtype=np.int32)\n",
    "# print(labels[1])\n",
    "# plt.imshow(train_data[1])\n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "model.train(np.asarray(train_data),np.asarray(labels))\n",
    "print('model is trained now!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sourabh/Documents/FR\n"
     ]
    }
   ],
   "source": [
    "cd /home/sourabh/Documents/FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img,size=0.5):\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray,1.3,5)\n",
    "\n",
    "    if faces is():\n",
    "        return img,[]\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "        roi = img[y:y+h,x:x+w]\n",
    "        roi = cv2.resize(roi,(200,200))\n",
    "\n",
    "    return img,roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{10: 'person_1', 1: 'Person_2'} 46.35973410729064\n",
      "{10: 'person_1', 1: 'Person_2'} 45.2889010810865\n",
      "{10: 'person_1', 1: 'Person_2'} 44.34273392061827\n",
      "{10: 'person_1', 1: 'Person_2'} 44.70738996229623\n",
      "{10: 'person_1', 1: 'Person_2'} 77.56318824229511\n",
      "{10: 'person_1', 1: 'Person_2'} 76.42856347406232\n",
      "{10: 'person_1', 1: 'Person_2'} 45.129692379122524\n",
      "{10: 'person_1', 1: 'Person_2'} 48.65659581595404\n",
      "{10: 'person_1', 1: 'Person_2'} 48.86327260766129\n",
      "{10: 'person_1', 1: 'Person_2'} 46.02743881181298\n",
      "{10: 'person_1', 1: 'Person_2'} 45.74561230136373\n",
      "{10: 'person_1', 1: 'Person_2'} 45.944366050190474\n",
      "{10: 'person_1', 1: 'Person_2'} 45.77019291119147\n",
      "{10: 'person_1', 1: 'Person_2'} 46.0682222345355\n",
      "{10: 'person_1', 1: 'Person_2'} 47.34825468521166\n",
      "{10: 'person_1', 1: 'Person_2'} 50.86446210066928\n",
      "{10: 'person_1', 1: 'Person_2'} 51.935234339182415\n",
      "{10: 'person_1', 1: 'Person_2'} 48.86627068747536\n",
      "{10: 'person_1', 1: 'Person_2'} 48.14979693042446\n",
      "{10: 'person_1', 1: 'Person_2'} 48.50167067657911\n",
      "{10: 'person_1', 1: 'Person_2'} 47.85016831780368\n",
      "{10: 'person_1', 1: 'Person_2'} 46.88951184654742\n",
      "{10: 'person_1', 1: 'Person_2'} 50.61877068399231\n",
      "{10: 'person_1', 1: 'Person_2'} 47.807202485964865\n",
      "{10: 'person_1', 1: 'Person_2'} 48.86435571789935\n",
      "{10: 'person_1', 1: 'Person_2'} 51.06315996218201\n",
      "{10: 'person_1', 1: 'Person_2'} 49.89045447598014\n",
      "{10: 'person_1', 1: 'Person_2'} 50.95068564744022\n",
      "{10: 'person_1', 1: 'Person_2'} 50.70460866176123\n",
      "{10: 'person_1', 1: 'Person_2'} 51.04978966500855\n",
      "{10: 'person_1', 1: 'Person_2'} 51.52941883257887\n",
      "{10: 'person_1', 1: 'Person_2'} 52.173479965971005\n",
      "{10: 'person_1', 1: 'Person_2'} 51.705764778537564\n",
      "{10: 'person_1', 1: 'Person_2'} 49.841662506846475\n",
      "{10: 'person_1', 1: 'Person_2'} 52.377108246156226\n",
      "{10: 'person_1', 1: 'Person_2'} 50.17170723185646\n",
      "{10: 'person_1', 1: 'Person_2'} 50.643680127846\n",
      "{10: 'person_1', 1: 'Person_2'} 50.153351982924306\n",
      "{10: 'person_1', 1: 'Person_2'} 49.281694784119985\n",
      "{10: 'person_1', 1: 'Person_2'} 50.1183294705468\n",
      "{10: 'person_1', 1: 'Person_2'} 50.2328373672762\n",
      "{10: 'person_1', 1: 'Person_2'} 50.87330209556981\n",
      "{10: 'person_1', 1: 'Person_2'} 48.482168249798605\n",
      "{10: 'person_1', 1: 'Person_2'} 48.21153847019978\n",
      "{10: 'person_1', 1: 'Person_2'} 48.94301746497868\n",
      "{10: 'person_1', 1: 'Person_2'} 49.57755073953643\n",
      "{10: 'person_1', 1: 'Person_2'} 48.82021208397751\n",
      "{10: 'person_1', 1: 'Person_2'} 49.18381926832763\n",
      "{10: 'person_1', 1: 'Person_2'} 47.40301220572103\n",
      "{10: 'person_1', 1: 'Person_2'} 49.80590450775241\n",
      "{10: 'person_1', 1: 'Person_2'} 47.60495303294405\n",
      "{10: 'person_1', 1: 'Person_2'} 48.631161522167304\n",
      "{10: 'person_1', 1: 'Person_2'} 50.27564125936758\n",
      "{10: 'person_1', 1: 'Person_2'} 47.7643822830774\n",
      "{10: 'person_1', 1: 'Person_2'} 49.22336830237432\n",
      "{10: 'person_1', 1: 'Person_2'} 49.65117936453145\n",
      "{10: 'person_1', 1: 'Person_2'} 47.71605400754029\n",
      "{10: 'person_1', 1: 'Person_2'} 48.32043462729221\n",
      "{10: 'person_1', 1: 'Person_2'} 47.4578136216735\n",
      "{10: 'person_1', 1: 'Person_2'} 48.51013605337194\n",
      "{10: 'person_1', 1: 'Person_2'} 49.1655874343449\n",
      "{10: 'person_1', 1: 'Person_2'} 50.25835987504247\n",
      "{10: 'person_1', 1: 'Person_2'} 49.704547508053416\n",
      "{10: 'person_1', 1: 'Person_2'} 47.19552289552934\n",
      "{10: 'person_1', 1: 'Person_2'} 49.03716530262004\n",
      "{10: 'person_1', 1: 'Person_2'} 49.14309424070217\n",
      "{10: 'person_1', 1: 'Person_2'} 48.57017245858807\n",
      "{10: 'person_1', 1: 'Person_2'} 47.89057450696804\n",
      "{10: 'person_1', 1: 'Person_2'} 47.46780402166941\n",
      "{10: 'person_1', 1: 'Person_2'} 49.30202738073074\n",
      "{10: 'person_1', 1: 'Person_2'} 47.22168209293661\n",
      "{10: 'person_1', 1: 'Person_2'} 46.82981014506124\n",
      "{10: 'person_1', 1: 'Person_2'} 48.81498751372865\n",
      "{10: 'person_1', 1: 'Person_2'} 47.58136542969195\n",
      "{10: 'person_1', 1: 'Person_2'} 49.252553168888745\n",
      "{10: 'person_1', 1: 'Person_2'} 48.6451951036436\n",
      "{10: 'person_1', 1: 'Person_2'} 47.88963561964628\n",
      "{10: 'person_1', 1: 'Person_2'} 47.89328582386046\n",
      "{10: 'person_1', 1: 'Person_2'} 49.05827710041305\n",
      "{10: 'person_1', 1: 'Person_2'} 47.6057302044894\n",
      "{10: 'person_1', 1: 'Person_2'} 48.478008738310486\n",
      "{10: 'person_1', 1: 'Person_2'} 49.95856321657946\n",
      "{10: 'person_1', 1: 'Person_2'} 48.07770260392369\n",
      "{10: 'person_1', 1: 'Person_2'} 48.90584709672027\n",
      "{10: 'person_1', 1: 'Person_2'} 47.9690882290812\n",
      "{10: 'person_1', 1: 'Person_2'} 48.65660776741576\n",
      "{10: 'person_1', 1: 'Person_2'} 48.88124267659285\n",
      "{10: 'person_1', 1: 'Person_2'} 48.28083828972504\n",
      "{10: 'person_1', 1: 'Person_2'} 48.58116319190719\n",
      "{10: 'person_1', 1: 'Person_2'} 48.30002876418397\n",
      "{10: 'person_1', 1: 'Person_2'} 48.02246654294348\n",
      "{10: 'person_1', 1: 'Person_2'} 47.77627888468678\n",
      "{10: 'person_1', 1: 'Person_2'} 48.00795555073914\n",
      "{10: 'person_1', 1: 'Person_2'} 48.57937140542061\n",
      "{10: 'person_1', 1: 'Person_2'} 47.61659416330881\n",
      "{10: 'person_1', 1: 'Person_2'} 48.98677768183836\n",
      "{10: 'person_1', 1: 'Person_2'} 48.12936511675852\n",
      "{10: 'person_1', 1: 'Person_2'} 47.60631650155244\n",
      "{10: 'person_1', 1: 'Person_2'} 49.557720923013676\n",
      "{10: 'person_1', 1: 'Person_2'} 49.132979767619375\n",
      "{10: 'person_1', 1: 'Person_2'} 49.76042400667827\n",
      "{10: 'person_1', 1: 'Person_2'} 50.73285735658585\n",
      "{10: 'person_1', 1: 'Person_2'} 48.30444975745042\n",
      "{10: 'person_1', 1: 'Person_2'} 48.97229562692573\n",
      "{10: 'person_1', 1: 'Person_2'} 49.06330052456481\n",
      "{10: 'person_1', 1: 'Person_2'} 47.373811589676016\n",
      "{10: 'person_1', 1: 'Person_2'} 50.05485807408821\n",
      "{10: 'person_1', 1: 'Person_2'} 49.6141745662342\n",
      "{10: 'person_1', 1: 'Person_2'} 48.19901726275242\n",
      "{10: 'person_1', 1: 'Person_2'} 49.436065971403764\n",
      "{10: 'person_1', 1: 'Person_2'} 48.43840232388411\n",
      "{10: 'person_1', 1: 'Person_2'} 48.27609963253526\n",
      "{10: 'person_1', 1: 'Person_2'} 50.90299552910537\n",
      "{10: 'person_1', 1: 'Person_2'} 49.33605046083692\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    image, face = face_detector(frame)\n",
    "\n",
    "    try:\n",
    "        face = cv2.cvtColor(face,cv2.COLOR_BGR2GRAY)\n",
    "        number, result = model.predict(face)\n",
    "        print(id_, result)\n",
    "#         cv2.imshow('face_crope',image)\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "        if result<500:\n",
    "            confidence = int(100*(1-(result)/300))\n",
    "            display_string = str(confidence)+'     '+ id_[number]\n",
    "        cv2.putText(image,display_string,(100,120),cv2.FONT_HERSHEY_COMPLEX,1,(250,120,255),2)\n",
    "\n",
    "        if confidence>75:\n",
    "            cv2.putText(image,\"unlocked\",(250,450),cv2.FONT_HERSHEY_COMPLEX,1,(0,255,0),2)\n",
    "            cv2.imshow('face cropper',image)\n",
    "\n",
    "        else:\n",
    "            cv2.putText(image,\"locked\",(250,450),cv2.FONT_HERSHEY_COMPLEX,1,(0,0,255),2)\n",
    "            cv2.imshow('face cropper',image)\n",
    "\n",
    "\n",
    "    except:\n",
    "            cv2.putText(image,\"face not found\",(250,450),cv2.FONT_HERSHEY_COMPLEX,1,(255,0,0),2)\n",
    "            cv2.imshow('face cropper',image)\n",
    "            pass\n",
    "\n",
    "    t = time.time()\n",
    "    t1=time.time()\n",
    "        \n",
    "    if cv2.waitKey(1)==13 or (t1-t)==60:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
